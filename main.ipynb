{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF702 Redes Neurais\n",
    "Este notebook contém um script base para o projeto da disciplina IF702 Redes Neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cross_validation import PredefinedSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leitura dos dados é feita utilizando a biblioteca `pandas`. O presente exemplo importa a base de dados `mammography`. Caso você esteja trabalhando com outro data set, modifique este trecho de código.\n",
    "Para importar o conjunto de dados do PAKDD, use a função `pd.read_table` ao invés da `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_table('data/TRN')\n",
    "data_set.drop_duplicates(inplace=True)  # Remove exemplos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135098</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281910</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225741</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480403</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDEX  UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE  SEXO_1  \\\n",
       "0      0     1     1     1     0     0     0     0  0.135098       1   \n",
       "1      1     1     0     1     0     0     1     0  0.273504       1   \n",
       "2      2     1     0     1     0     0     1     0  0.281910       0   \n",
       "3      3     1     1     1     0     0     0     0  0.225741       0   \n",
       "4      4     1     1     0     0     0     1     0  0.480403       0   \n",
       "\n",
       "      ...       CEP4_7  CEP4_8  CEP4_9  CEP4_10  CEP4_11  CEP4_12  CEP4_13  \\\n",
       "0     ...            0       0       1        1        0        1        1   \n",
       "1     ...            0       1       0        1        1        0        0   \n",
       "2     ...            1       1       0        0        0        0        1   \n",
       "3     ...            1       1       0        1        1        0        1   \n",
       "4     ...            1       1       1        0        0        1        0   \n",
       "\n",
       "   CEP4_14  IND_BOM_1_1  IND_BOM_1_2  \n",
       "0        1            0            1  \n",
       "1        0            1            0  \n",
       "2        0            1            0  \n",
       "3        0            1            0  \n",
       "4        1            1            0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibe as 5 primeiras linhas do data set\n",
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>389196.00000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>3.891960e+05</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>0.889274</td>\n",
       "      <td>0.691952</td>\n",
       "      <td>0.476552</td>\n",
       "      <td>0.296195</td>\n",
       "      <td>0.241179</td>\n",
       "      <td>0.218011</td>\n",
       "      <td>0.186836</td>\n",
       "      <td>4.552049e-01</td>\n",
       "      <td>0.521514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423378</td>\n",
       "      <td>0.417540</td>\n",
       "      <td>0.425708</td>\n",
       "      <td>0.459820</td>\n",
       "      <td>0.440842</td>\n",
       "      <td>0.436896</td>\n",
       "      <td>0.433709</td>\n",
       "      <td>0.440339</td>\n",
       "      <td>0.655449</td>\n",
       "      <td>0.344551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>112351.35202</td>\n",
       "      <td>0.313793</td>\n",
       "      <td>0.461687</td>\n",
       "      <td>0.499451</td>\n",
       "      <td>0.456579</td>\n",
       "      <td>0.427799</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.389781</td>\n",
       "      <td>2.537459e-01</td>\n",
       "      <td>0.499538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494095</td>\n",
       "      <td>0.493154</td>\n",
       "      <td>0.494451</td>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.496489</td>\n",
       "      <td>0.496002</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.496428</td>\n",
       "      <td>0.475222</td>\n",
       "      <td>0.475222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.506237e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>97298.75000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.507866e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.375241e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>291896.25000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.578835e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>389195.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              INDEX           UF_1           UF_2           UF_3  \\\n",
       "count  389196.00000  389196.000000  389196.000000  389196.000000   \n",
       "mean   194597.50000       0.889274       0.691952       0.476552   \n",
       "std    112351.35202       0.313793       0.461687       0.499451   \n",
       "min         0.00000       0.000000       0.000000       0.000000   \n",
       "25%     97298.75000       1.000000       0.000000       0.000000   \n",
       "50%    194597.50000       1.000000       1.000000       0.000000   \n",
       "75%    291896.25000       1.000000       1.000000       1.000000   \n",
       "max    389195.00000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                UF_4           UF_5           UF_6           UF_7  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.296195       0.241179       0.218011       0.186836   \n",
       "std         0.456579       0.427799       0.412895       0.389781   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              IDADE         SEXO_1      ...               CEP4_7  \\\n",
       "count  3.891960e+05  389196.000000      ...        389196.000000   \n",
       "mean   4.552049e-01       0.521514      ...             0.423378   \n",
       "std    2.537459e-01       0.499538      ...             0.494095   \n",
       "min    5.506237e-16       0.000000      ...             0.000000   \n",
       "25%    2.507866e-01       0.000000      ...             0.000000   \n",
       "50%    4.375241e-01       1.000000      ...             0.000000   \n",
       "75%    6.578835e-01       1.000000      ...             1.000000   \n",
       "max    1.000000e+00       1.000000      ...             1.000000   \n",
       "\n",
       "              CEP4_8         CEP4_9        CEP4_10        CEP4_11  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.417540       0.425708       0.459820       0.440842   \n",
       "std         0.493154       0.494451       0.498384       0.496489   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             CEP4_12        CEP4_13        CEP4_14    IND_BOM_1_1  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.436896       0.433709       0.440339       0.655449   \n",
       "std         0.496002       0.495587       0.496428       0.475222   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       1.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         IND_BOM_1_2  \n",
       "count  389196.000000  \n",
       "mean        0.344551  \n",
       "std         0.475222  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 246 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estatísticas sobre as variáveis\n",
    "data_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos separar o data set em atributos dependentes (X = features) e independentes (y = classe). No caso do `mammography` a classe majoritária está codificada como -1 e a classe minoritária está codificada como 1. Para treinar nossa rede neural precisamos que os valores de classe sejam 0 e 1 (saída da última camada é uma sigmóide), assim modificamos a codificação da majoritária para 0.\n",
    "\n",
    "Perceba que esse pré-processamento varia de data set para data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_set.drop(['INDEX', 'IND_BOM_1_1', 'IND_BOM_1_2'], axis=1)\n",
    "y = data_set[['IND_BOM_1_1', 'IND_BOM_1_2']]\n",
    "\n",
    "# IND_BOM_1_1 e IND_BOM_1_2 sao os indicadores de \"bom pagador\", o que se refere a conceder o credito\n",
    "# ou nao. Ou seja, a resposta de uma vai ser o inverso da outra. \n",
    "\n",
    "# Vamos utilzizar apenas uma como target, pois ambas representam \"a mesma coisa\".\n",
    "\n",
    "y = y.drop(['IND_BOM_1_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão dos Dados em Treino, Validação, e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui dividimos o data set em treino, validação e teste de maneira estratificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    127549\n",
      "0     67049\n",
      "Name: IND_BOM_1_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Treino: 50%, Validação: 25%, Teste: 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, \n",
    "                                                    random_state=42, shuffle=True, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, \n",
    "                                                  random_state=42, shuffle=True, stratify=y_train)\n",
    "\n",
    "print(y_train['IND_BOM_1_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling dos Dados e Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o comportamento da rede com diferentes funções de sampling, as mesmas devem ser implementadas e aplicadas ao conjunto de treinamento antes da normalização dos dados (você também pode investigar qual o efeito de aplicar o sampling após a normalização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IND_BOM_1_1\n",
      "0     67049\n",
      "1    127549\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de treinamento\n",
      "1    127549\n",
      "0    127549\n",
      "Name: IND_BOM_1_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## TO DO -- Implementar as funções de sampling a serem utilizadas\n",
    "\n",
    "y_train_group = y_train.groupby('IND_BOM_1_1')\n",
    "print(y_train_group.size())\n",
    "\n",
    "X_train_columns = X_train.columns\n",
    "\n",
    "# oversampling nos dados de treinamento\n",
    "X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "\n",
    "y_train = pd.DataFrame(np.array(y_train), columns=['IND_BOM_1_1'])\n",
    "X_train = pd.DataFrame(np.array(X_train), columns=X_train_columns)\n",
    "\n",
    "print(\"Base de treinamento\")\n",
    "print(y_train['IND_BOM_1_1'].value_counts())\n",
    "\n",
    "# oversampling nos dados de validação\n",
    "#X_val, y_val = SMOTE().fit_sample(X_val, y_val)\n",
    "#print(\"Base de validação\")\n",
    "#print(\"Class 0: \", np.where(y_val == 0)[0].shape[0])\n",
    "#print(\"Class 1: \", np.where(y_val == 1)[0].shape[0])\n",
    "\n",
    "#y_#val = pd.DataFrame(np.array(y_val), columns=['IND_BOM_1_1'])\n",
    "#X_v#al = pd.DataFrame(np.array(X_val), columns=X_train_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.array(X_train), columns=X_train_columns)\n",
    "X_train.to_csv('X_train_sampling.csv', index=False)\n",
    "y_train.to_csv('y_train_sampling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante lembrar de normalizar os dados. A classe `StandardScaler` centraliza as variáveis e transforma as features para terem variância unitária. Você pode testar outras opções como o `MinMaxScaler`.\n",
    "\n",
    "Todas as alternativas estão disponíveis em:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição e Treino da Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos a arquitetura da nossa rede neural e treinamos ela.\n",
    "\n",
    "No presente exemplo a rede possui apenas uma camada escondida. O código é bem intuitivo e a adição de novas camadas pode ser feita através da função `add`.\n",
    "\n",
    "Para treinar a rede várias funções de otimização estão disponíveis. \n",
    "\n",
    "Confira os exemplos em: https://keras.io/optimizers/\n",
    "\n",
    "O treinamento da rede pode ser interrompido baseado na performance dela em um conjunto de validação através de callbacks.\n",
    "\n",
    "Confira a documentação da classe `EarlyStopping`: https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de features do nosso data set.\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "# Agora adicionamos a primeira camada escondida contendo 16 neurônios e função de ativação\n",
    "# tangente hiperbólica. Por ser a primeira camada adicionada à rede, precisamos especificar\n",
    "# a dimensão de entrada (número de features do data set).\n",
    "classifier.add(Dense(16, activation='tanh', input_dim=input_dim))\n",
    "\n",
    "# Em seguida adicionamos a camada de saída. Como nosso problema é binário só precisamos de\n",
    "# 1 neurônio com função de ativação sigmoidal. A partir da segunda camada adicionada keras já\n",
    "# consegue inferir o número de neurônios de entrada (16) e nós não precisamos mais especificar.\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Por fim compilamos o modelo especificando um otimizador, a função de custo, e opcionalmente\n",
    "# métricas para serem observadas durante treinamento.\n",
    "classifier.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3924 samples, validate on 1962 samples\n",
      "Epoch 1/100000\n",
      "3924/3924 [==============================] - 1s 369us/step - loss: 0.2219 - val_loss: 0.1736\n",
      "Epoch 2/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.1418 - val_loss: 0.1149\n",
      "Epoch 3/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0925 - val_loss: 0.0745\n",
      "Epoch 4/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0603 - val_loss: 0.0506\n",
      "Epoch 5/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0423 - val_loss: 0.0378\n",
      "Epoch 6/100000\n",
      "3924/3924 [==============================] - 0s 32us/step - loss: 0.0327 - val_loss: 0.0310\n",
      "Epoch 7/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0274 - val_loss: 0.0271\n",
      "Epoch 8/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0242 - val_loss: 0.0248\n",
      "Epoch 9/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 10/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 11/100000\n",
      "3924/3924 [==============================] - 0s 53us/step - loss: 0.0197 - val_loss: 0.0214\n",
      "Epoch 12/100000\n",
      "3924/3924 [==============================] - 0s 41us/step - loss: 0.0189 - val_loss: 0.0207\n",
      "Epoch 13/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0183 - val_loss: 0.0202\n",
      "Epoch 14/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0178 - val_loss: 0.0199\n",
      "Epoch 15/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 16/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0170 - val_loss: 0.0192\n",
      "Epoch 17/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0167 - val_loss: 0.0190\n",
      "Epoch 18/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0165 - val_loss: 0.0188\n",
      "Epoch 19/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 20/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0161 - val_loss: 0.0184\n",
      "Epoch 21/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0158 - val_loss: 0.0183\n",
      "Epoch 22/100000\n",
      "3924/3924 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0182\n",
      "Epoch 23/100000\n",
      "3924/3924 [==============================] - 0s 51us/step - loss: 0.0155 - val_loss: 0.0180\n",
      "Epoch 24/100000\n",
      "3924/3924 [==============================] - 0s 48us/step - loss: 0.0154 - val_loss: 0.0179\n",
      "Epoch 25/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 26/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0152 - val_loss: 0.0177\n",
      "Epoch 27/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0150 - val_loss: 0.0176\n",
      "Epoch 28/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0149 - val_loss: 0.0175\n",
      "Epoch 29/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0148 - val_loss: 0.0174\n",
      "Epoch 30/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0146 - val_loss: 0.0173\n",
      "Epoch 31/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0146 - val_loss: 0.0172\n",
      "Epoch 32/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0145 - val_loss: 0.0171\n",
      "Epoch 33/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0171\n",
      "Epoch 34/100000\n",
      "3924/3924 [==============================] - 0s 44us/step - loss: 0.0143 - val_loss: 0.0170\n",
      "Epoch 35/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0142 - val_loss: 0.0169\n",
      "Epoch 36/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0142 - val_loss: 0.0169\n",
      "Epoch 37/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 38/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0139 - val_loss: 0.0167\n",
      "Epoch 39/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0139 - val_loss: 0.0167\n",
      "Epoch 40/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0138 - val_loss: 0.0166\n",
      "Epoch 41/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0138 - val_loss: 0.0166\n",
      "Epoch 42/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0137 - val_loss: 0.0165\n",
      "Epoch 43/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0166\n",
      "Epoch 44/100000\n",
      "3924/3924 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0165\n",
      "Epoch 45/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 46/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 47/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0134 - val_loss: 0.0164\n",
      "Epoch 48/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0134 - val_loss: 0.0163\n",
      "Epoch 49/100000\n",
      "3924/3924 [==============================] - 0s 41us/step - loss: 0.0133 - val_loss: 0.0163\n",
      "Epoch 50/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 51/100000\n",
      "3924/3924 [==============================] - 0s 51us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 52/100000\n",
      "3924/3924 [==============================] - 0s 43us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 53/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 54/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 55/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 56/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0130 - val_loss: 0.0161\n",
      "Epoch 57/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0129 - val_loss: 0.0162\n",
      "Epoch 58/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0128 - val_loss: 0.0160\n",
      "Epoch 59/100000\n",
      "3924/3924 [==============================] - 0s 33us/step - loss: 0.0128 - val_loss: 0.0161\n",
      "Epoch 60/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 61/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 62/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 63/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0127 - val_loss: 0.0159\n",
      "Epoch 64/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 65/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0126 - val_loss: 0.0160\n",
      "Epoch 66/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0127 - val_loss: 0.0159\n",
      "Epoch 67/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 68/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 69/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 70/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 71/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 72/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 73/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 74/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0124 - val_loss: 0.0157\n",
      "Epoch 75/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0124 - val_loss: 0.0157\n",
      "Epoch 76/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 77/100000\n",
      "3924/3924 [==============================] - 0s 32us/step - loss: 0.0123 - val_loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "# Para treinar a rede passamos o conjunto de treinamento e especificamos o tamanho do mini-batch,\n",
    "# o número máximo de épocas, e opcionalmente callbacks. No seguinte exemplo utilizamos early\n",
    "# stopping para interromper o treinamento caso a performance não melhore em um conjunto de validação.\n",
    "history = classifier.fit(X_train, y_train, batch_size=64, epochs=100000, \n",
    "                         callbacks=[EarlyStopping(patience=3)], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "classifiers.append(('Random Forest', forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "classifiers.append(('SVM', svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "g_boosting = GradientBoostingClassifier()\n",
    "g_boosting.fit(X_train, y_train)\n",
    "\n",
    "classifiers.append(('Gradient Boosting', g_boosting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "classifiers.append(('MLP', mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "\n",
      "Accuracy:         0.6242\n",
      "Recall:           0.6937\n",
      "Precision:        0.7220\n",
      "F1:               0.7076\n",
      "AUROC:            0.6364\n",
      "AUPR:             0.7506\n",
      "\n",
      "\n",
      "MLP\n",
      "\n",
      "Accuracy:         0.6277\n",
      "Recall:           0.7216\n",
      "Precision:        0.7137\n",
      "F1:               0.7176\n",
      "AUROC:            0.6297\n",
      "AUPR:             0.7526\n",
      "\n",
      "\n",
      "SVM\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-aec3c66aa3a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassfier\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassfier\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_pred_val_scores\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mclassfier\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauroc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maupr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_performance_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint_metrics_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauroc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maupr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \"\"\"\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    558\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "for classfier in classifiers:\n",
    "    print(classfier[0])\n",
    "    y_pred_val = classfier[1].predict(X_val)\n",
    "    y_pred_val_scores =  classfier[1].predict_proba(X_val)[:, 1]\n",
    "    accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_val, y_pred_val_scores)\n",
    "    print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:         0.6279\n",
      "Recall:           0.6234\n",
      "Precision:        0.7654\n",
      "F1:               0.6871\n",
      "AUROC:            0.6777\n",
      "AUPR:             0.7881\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:         0.6174\n",
      "Recall:           0.6890\n",
      "Precision:        0.7164\n",
      "F1:               0.7024\n",
      "AUROC:            0.6245\n",
      "AUPR:             0.7485\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6239\n",
      "Recall:           0.7178\n",
      "Precision:        0.7111\n",
      "F1:               0.7144\n",
      "AUROC:            0.6239\n",
      "AUPR:             0.7490\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6234\n",
      "Recall:           0.6796\n",
      "Precision:        0.7278\n",
      "F1:               0.7029\n",
      "AUROC:            0.6406\n",
      "AUPR:             0.7607\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6126\n",
      "Recall:           0.6863\n",
      "Precision:        0.7122\n",
      "F1:               0.6990\n",
      "AUROC:            0.6145\n",
      "AUPR:             0.7411\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6096\n",
      "Recall:           0.6816\n",
      "Precision:        0.7109\n",
      "F1:               0.6959\n",
      "AUROC:            0.6138\n",
      "AUPR:             0.7407\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6359\n",
      "Recall:           0.7193\n",
      "Precision:        0.7236\n",
      "F1:               0.7214\n",
      "AUROC:            0.6454\n",
      "AUPR:             0.7644\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6401\n",
      "Recall:           0.7613\n",
      "Precision:        0.7103\n",
      "F1:               0.7349\n",
      "AUROC:            0.6356\n",
      "AUPR:             0.7572\n",
      "{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6116\n",
      "Recall:           0.6775\n",
      "Precision:        0.7150\n",
      "F1:               0.6957\n",
      "AUROC:            0.6205\n",
      "AUPR:             0.7467\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6429\n",
      "Recall:           0.6583\n",
      "Precision:        0.7642\n",
      "F1:               0.7073\n",
      "AUROC:            0.6871\n",
      "AUPR:             0.7988\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6415\n",
      "Recall:           0.7292\n",
      "Precision:        0.7254\n",
      "F1:               0.7273\n",
      "AUROC:            0.6538\n",
      "AUPR:             0.7723\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6354\n",
      "Recall:           0.7324\n",
      "Precision:        0.7173\n",
      "F1:               0.7248\n",
      "AUROC:            0.6410\n",
      "AUPR:             0.7629\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6310\n",
      "Recall:           0.6805\n",
      "Precision:        0.7365\n",
      "F1:               0.7074\n",
      "AUROC:            0.6556\n",
      "AUPR:             0.7736\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6259\n",
      "Recall:           0.6895\n",
      "Precision:        0.7260\n",
      "F1:               0.7073\n",
      "AUROC:            0.6405\n",
      "AUPR:             0.7610\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6263\n",
      "Recall:           0.6908\n",
      "Precision:        0.7259\n",
      "F1:               0.7079\n",
      "AUROC:            0.6407\n",
      "AUPR:             0.7617\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6368\n",
      "Recall:           0.7059\n",
      "Precision:        0.7308\n",
      "F1:               0.7181\n",
      "AUROC:            0.6550\n",
      "AUPR:             0.7725\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6365\n",
      "Recall:           0.7129\n",
      "Precision:        0.7272\n",
      "F1:               0.7200\n",
      "AUROC:            0.6509\n",
      "AUPR:             0.7698\n",
      "{'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6351\n",
      "Recall:           0.7376\n",
      "Precision:        0.7148\n",
      "F1:               0.7260\n",
      "AUROC:            0.6382\n",
      "AUPR:             0.7613\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6554\n",
      "Recall:           0.9995\n",
      "Precision:        0.6555\n",
      "F1:               0.7918\n",
      "AUROC:            0.5564\n",
      "AUPR:             0.6975\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6271\n",
      "Recall:           0.7145\n",
      "Precision:        0.7160\n",
      "F1:               0.7153\n",
      "AUROC:            0.6359\n",
      "AUPR:             0.7649\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'logistic', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6168\n",
      "Recall:           0.7030\n",
      "Precision:        0.7096\n",
      "F1:               0.7063\n",
      "AUROC:            0.6253\n",
      "AUPR:             0.7587\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6168\n",
      "Recall:           0.6886\n",
      "Precision:        0.7160\n",
      "F1:               0.7020\n",
      "AUROC:            0.6247\n",
      "AUPR:             0.7516\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6060\n",
      "Recall:           0.6633\n",
      "Precision:        0.7150\n",
      "F1:               0.6882\n",
      "AUROC:            0.6161\n",
      "AUPR:             0.7449\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'tanh', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6168\n",
      "Recall:           0.6687\n",
      "Precision:        0.7252\n",
      "F1:               0.6958\n",
      "AUROC:            0.6367\n",
      "AUPR:             0.7628\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.001}\n",
      "\n",
      "Accuracy:         0.6305\n",
      "Recall:           0.7389\n",
      "Precision:        0.7094\n",
      "F1:               0.7238\n",
      "AUROC:            0.6295\n",
      "AUPR:             0.7569\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.01}\n",
      "\n",
      "Accuracy:         0.6294\n",
      "Recall:           0.7478\n",
      "Precision:        0.7048\n",
      "F1:               0.7257\n",
      "AUROC:            0.6246\n",
      "AUPR:             0.7505\n",
      "{'hidden_layer_sizes': (100, 25, 10), 'activation': 'relu', 'solver': 'sgd', 'learning_rate_init': 0.1}\n",
      "\n",
      "Accuracy:         0.6151\n",
      "Recall:           0.6628\n",
      "Precision:        0.7261\n",
      "F1:               0.6930\n",
      "AUROC:            0.6292\n",
      "AUPR:             0.7547\n"
     ]
    }
   ],
   "source": [
    "experiments_params = {'hidden_layer_sizes' : [(100,), (50,), (100, 25, 10)], 'activation' : ['logistic', 'tanh', 'relu'], 'solver' : ['sgd'], 'learning_rate_init' : [0.001, 0.01, 0.1]}\n",
    "\n",
    "keys, values = zip(*experiments_params.items())\n",
    "for v in itertools.product(*values):\n",
    "    params = dict(zip(keys, v))\n",
    "    print(params)\n",
    "    \n",
    "    mlp = MLPClassifier(**params)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_val = mlp.predict(X_val)\n",
    "    y_pred_val_scores =  mlp.predict_proba(X_val)[:, 1]\n",
    "    accuracy, recall, precision, f1, auroc, aupr = compute_performance_metrics(y_val, y_pred_val, y_pred_val_scores)\n",
    "    print_metrics_summary(accuracy, recall, precision, f1, auroc, aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas funções auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o melhor loss de treino e validação.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    Dicionário contendo o melhor loss de treino e de validação baseado \n",
    "    no menor loss de validação.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    \"\"\"Função para plotar as curvas de erro do treinamento da rede neural.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    A função gera o gráfico do treino da rede e retorna None.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
    "    print()\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
    "    if auroc is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
    "    if aupr is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_error_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
