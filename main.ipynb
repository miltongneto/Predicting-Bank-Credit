{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF702 Redes Neurais\n",
    "Este notebook contém um script base para o projeto da disciplina IF702 Redes Neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allyson/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leitura dos dados é feita utilizando a biblioteca `pandas`. O presente exemplo importa a base de dados `mammography`. Caso você esteja trabalhando com outro data set, modifique este trecho de código.\n",
    "Para importar o conjunto de dados do PAKDD, use a função `pd.read_table` ao invés da `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_table('data/TRN.csv')\n",
    "data_set.drop_duplicates(inplace=True)  # Remove exemplos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135098</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281910</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225741</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480403</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDEX  UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE  SEXO_1  \\\n",
       "0      0     1     1     1     0     0     0     0  0.135098       1   \n",
       "1      1     1     0     1     0     0     1     0  0.273504       1   \n",
       "2      2     1     0     1     0     0     1     0  0.281910       0   \n",
       "3      3     1     1     1     0     0     0     0  0.225741       0   \n",
       "4      4     1     1     0     0     0     1     0  0.480403       0   \n",
       "\n",
       "      ...       CEP4_7  CEP4_8  CEP4_9  CEP4_10  CEP4_11  CEP4_12  CEP4_13  \\\n",
       "0     ...            0       0       1        1        0        1        1   \n",
       "1     ...            0       1       0        1        1        0        0   \n",
       "2     ...            1       1       0        0        0        0        1   \n",
       "3     ...            1       1       0        1        1        0        1   \n",
       "4     ...            1       1       1        0        0        1        0   \n",
       "\n",
       "   CEP4_14  IND_BOM_1_1  IND_BOM_1_2  \n",
       "0        1            0            1  \n",
       "1        0            1            0  \n",
       "2        0            1            0  \n",
       "3        0            1            0  \n",
       "4        1            1            0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibe as 5 primeiras linhas do data set\n",
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>UF_1</th>\n",
       "      <th>UF_2</th>\n",
       "      <th>UF_3</th>\n",
       "      <th>UF_4</th>\n",
       "      <th>UF_5</th>\n",
       "      <th>UF_6</th>\n",
       "      <th>UF_7</th>\n",
       "      <th>IDADE</th>\n",
       "      <th>SEXO_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CEP4_7</th>\n",
       "      <th>CEP4_8</th>\n",
       "      <th>CEP4_9</th>\n",
       "      <th>CEP4_10</th>\n",
       "      <th>CEP4_11</th>\n",
       "      <th>CEP4_12</th>\n",
       "      <th>CEP4_13</th>\n",
       "      <th>CEP4_14</th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "      <th>IND_BOM_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>389196.00000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>3.891960e+05</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "      <td>389196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>0.889274</td>\n",
       "      <td>0.691952</td>\n",
       "      <td>0.476552</td>\n",
       "      <td>0.296195</td>\n",
       "      <td>0.241179</td>\n",
       "      <td>0.218011</td>\n",
       "      <td>0.186836</td>\n",
       "      <td>4.552049e-01</td>\n",
       "      <td>0.521514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423378</td>\n",
       "      <td>0.417540</td>\n",
       "      <td>0.425708</td>\n",
       "      <td>0.459820</td>\n",
       "      <td>0.440842</td>\n",
       "      <td>0.436896</td>\n",
       "      <td>0.433709</td>\n",
       "      <td>0.440339</td>\n",
       "      <td>0.655449</td>\n",
       "      <td>0.344551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>112351.35202</td>\n",
       "      <td>0.313793</td>\n",
       "      <td>0.461687</td>\n",
       "      <td>0.499451</td>\n",
       "      <td>0.456579</td>\n",
       "      <td>0.427799</td>\n",
       "      <td>0.412895</td>\n",
       "      <td>0.389781</td>\n",
       "      <td>2.537459e-01</td>\n",
       "      <td>0.499538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494095</td>\n",
       "      <td>0.493154</td>\n",
       "      <td>0.494451</td>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.496489</td>\n",
       "      <td>0.496002</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.496428</td>\n",
       "      <td>0.475222</td>\n",
       "      <td>0.475222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.506237e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>97298.75000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.507866e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>194597.50000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.375241e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>291896.25000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.578835e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>389195.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              INDEX           UF_1           UF_2           UF_3  \\\n",
       "count  389196.00000  389196.000000  389196.000000  389196.000000   \n",
       "mean   194597.50000       0.889274       0.691952       0.476552   \n",
       "std    112351.35202       0.313793       0.461687       0.499451   \n",
       "min         0.00000       0.000000       0.000000       0.000000   \n",
       "25%     97298.75000       1.000000       0.000000       0.000000   \n",
       "50%    194597.50000       1.000000       1.000000       0.000000   \n",
       "75%    291896.25000       1.000000       1.000000       1.000000   \n",
       "max    389195.00000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                UF_4           UF_5           UF_6           UF_7  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.296195       0.241179       0.218011       0.186836   \n",
       "std         0.456579       0.427799       0.412895       0.389781   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              IDADE         SEXO_1      ...               CEP4_7  \\\n",
       "count  3.891960e+05  389196.000000      ...        389196.000000   \n",
       "mean   4.552049e-01       0.521514      ...             0.423378   \n",
       "std    2.537459e-01       0.499538      ...             0.494095   \n",
       "min    5.506237e-16       0.000000      ...             0.000000   \n",
       "25%    2.507866e-01       0.000000      ...             0.000000   \n",
       "50%    4.375241e-01       1.000000      ...             0.000000   \n",
       "75%    6.578835e-01       1.000000      ...             1.000000   \n",
       "max    1.000000e+00       1.000000      ...             1.000000   \n",
       "\n",
       "              CEP4_8         CEP4_9        CEP4_10        CEP4_11  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.417540       0.425708       0.459820       0.440842   \n",
       "std         0.493154       0.494451       0.498384       0.496489   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             CEP4_12        CEP4_13        CEP4_14    IND_BOM_1_1  \\\n",
       "count  389196.000000  389196.000000  389196.000000  389196.000000   \n",
       "mean        0.436896       0.433709       0.440339       0.655449   \n",
       "std         0.496002       0.495587       0.496428       0.475222   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       1.000000   \n",
       "75%         1.000000       1.000000       1.000000       1.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         IND_BOM_1_2  \n",
       "count  389196.000000  \n",
       "mean        0.344551  \n",
       "std         0.475222  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 246 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estatísticas sobre as variáveis\n",
    "data_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos separar o data set em atributos dependentes (X = features) e independentes (y = classe). No caso do `mammography` a classe majoritária está codificada como -1 e a classe minoritária está codificada como 1. Para treinar nossa rede neural precisamos que os valores de classe sejam 0 e 1 (saída da última camada é uma sigmóide), assim modificamos a codificação da majoritária para 0.\n",
    "\n",
    "Perceba que esse pré-processamento varia de data set para data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_set.drop(['INDEX', 'IND_BOM_1_1', 'IND_BOM_1_2'], axis=1)\n",
    "y = data_set[['IND_BOM_1_1', 'IND_BOM_1_2']]\n",
    "\n",
    "# IND_BOM_1_1 e IND_BOM_1_2 sao os indicadores de \"bom pagador\", o que se refere a conceder o credito\n",
    "# ou nao. Ou seja, a resposta de uma vai ser o inverso da outra. \n",
    "\n",
    "# Vamos utilzizar apenas uma como target, pois ambas representam \"a mesma coisa\".\n",
    "\n",
    "y = y.drop(['IND_BOM_1_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão dos Dados em Treino, Validação, e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui dividimos o data set em treino, validação e teste de maneira estratificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 67049, class 1: 127549\n"
     ]
    }
   ],
   "source": [
    "## Treino: 50%, Validação: 25%, Teste: 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, \n",
    "                                                    random_state=42, shuffle=True, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, \n",
    "                                                  random_state=42, shuffle=True, stratify=y_train)\n",
    "\n",
    "train_class0 = y_train[y_train == 0] \n",
    "train_class0 = train_class0.dropna()\n",
    "train_class1 = y_train[y_train == 1] \n",
    "train_class1 = train_class1.dropna()\n",
    "\n",
    "print(\"class 0: {}, class 1: {}\".format(train_class0.shape[0], train_class1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling dos Dados e Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o comportamento da rede com diferentes funções de sampling, as mesmas devem ser implementadas e aplicadas ao conjunto de treinamento antes da normalização dos dados (você também pode investigar qual o efeito de aplicar o sampling após a normalização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IND_BOM_1_1\n",
      "0     67049\n",
      "1    127549\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allyson/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de teste\n",
      "Class 0:  127549\n",
      "Class 1:  127549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allyson/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de validação\n",
      "Class 0:  63775\n",
      "Class 1:  63775\n",
      "        UF_1  UF_2  UF_3  UF_4  UF_5  UF_6  UF_7     IDADE    SEXO_1  \\\n",
      "0        1.0   0.0   0.0   0.0   1.0   1.0   0.0  0.921349  0.000000   \n",
      "1        1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.879833  1.000000   \n",
      "2        0.0   1.0   1.0   0.0   1.0   0.0   0.0  0.521749  1.000000   \n",
      "3        1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.095115  1.000000   \n",
      "4        1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.818269  0.000000   \n",
      "5        1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.274413  1.000000   \n",
      "6        1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.718482  1.000000   \n",
      "7        1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.890226  1.000000   \n",
      "8        1.0   0.0   1.0   1.0   0.0   0.0   0.0  0.076487  1.000000   \n",
      "9        1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.931118  1.000000   \n",
      "10       1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.644935  0.000000   \n",
      "11       1.0   1.0   0.0   0.0   0.0   0.0   1.0  0.012367  1.000000   \n",
      "12       1.0   1.0   0.0   0.0   0.0   1.0   0.0  0.564628  1.000000   \n",
      "13       0.0   1.0   1.0   1.0   0.0   0.0   0.0  0.118287  1.000000   \n",
      "14       1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.905390  1.000000   \n",
      "15       1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.640050  0.000000   \n",
      "16       0.0   1.0   1.0   0.0   1.0   0.0   0.0  0.328083  1.000000   \n",
      "17       1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.867679  0.000000   \n",
      "18       1.0   1.0   0.0   0.0   0.0   0.0   1.0  0.476201  1.000000   \n",
      "19       1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.163608  1.000000   \n",
      "20       1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.342224  0.000000   \n",
      "21       1.0   0.0   1.0   0.0   1.0   0.0   0.0  0.247947  1.000000   \n",
      "22       1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.584222  1.000000   \n",
      "23       1.0   0.0   0.0   1.0   0.0   0.0   1.0  0.729955  1.000000   \n",
      "24       1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.752161  1.000000   \n",
      "25       1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.557870  0.000000   \n",
      "26       1.0   1.0   0.0   0.0   0.0   1.0   0.0  0.526179  1.000000   \n",
      "27       0.0   1.0   1.0   1.0   0.0   0.0   0.0  0.302242  0.000000   \n",
      "28       1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.137483  1.000000   \n",
      "29       1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.261861  1.000000   \n",
      "...      ...   ...   ...   ...   ...   ...   ...       ...       ...   \n",
      "255068   1.0   1.0   0.0   0.0   0.0   1.0   0.0  0.269439  0.530307   \n",
      "255069   1.0   0.0   1.0   0.0   0.0   0.0   1.0  0.512492  1.000000   \n",
      "255070   1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.330363  0.000000   \n",
      "255071   0.0   1.0   1.0   0.0   1.0   0.0   0.0  0.307023  0.239828   \n",
      "255072   1.0   0.0   0.0   0.0   1.0   0.0   1.0  0.237606  1.000000   \n",
      "255073   1.0   0.0   0.0   0.0   1.0   1.0   0.0  0.178331  1.000000   \n",
      "255074   1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.384317  0.238716   \n",
      "255075   1.0   0.0   1.0   0.0   1.0   0.0   0.0  0.493572  0.000000   \n",
      "255076   1.0   1.0   0.0   0.0   0.0   1.0   0.0  0.465244  0.243699   \n",
      "255077   1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.644672  0.864516   \n",
      "255078   1.0   0.0   0.0   1.0   0.0   0.0   1.0  0.560192  1.000000   \n",
      "255079   1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.055056  0.770523   \n",
      "255080   1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.692171  1.000000   \n",
      "255081   1.0   0.0   1.0   1.0   0.0   0.0   0.0  0.349177  1.000000   \n",
      "255082   1.0   0.0   1.0   0.0   1.0   0.0   0.0  0.462630  1.000000   \n",
      "255083   1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.356842  0.136778   \n",
      "255084   0.0   1.0   1.0   1.0   0.0   0.0   0.0  0.566485  0.000000   \n",
      "255085   1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.366434  0.884022   \n",
      "255086   1.0   1.0   0.0   0.0   1.0   0.0   0.0  0.447316  0.162447   \n",
      "255087   1.0   0.0   1.0   1.0   0.0   0.0   0.0  0.299593  0.913566   \n",
      "255088   1.0   0.0   1.0   0.0   1.0   0.0   0.0  0.107710  1.000000   \n",
      "255089   1.0   0.0   0.0   1.0   0.0   1.0   0.0  0.116381  1.000000   \n",
      "255090   1.0   0.0   1.0   0.0   0.0   1.0   0.0  0.529892  0.000000   \n",
      "255091   1.0   0.0   1.0   1.0   0.0   0.0   0.0  0.506404  1.000000   \n",
      "255092   1.0   0.0   0.0   1.0   0.0   1.0   0.0  0.418164  0.014315   \n",
      "255093   1.0   1.0   0.0   0.0   0.0   1.0   0.0  0.177705  0.000000   \n",
      "255094   1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.092584  1.000000   \n",
      "255095   1.0   1.0   1.0   0.0   0.0   0.0   0.0  0.379880  1.000000   \n",
      "255096   1.0   0.0   0.0   0.0   1.0   1.0   0.0  0.273128  0.451621   \n",
      "255097   1.0   1.0   0.0   1.0   0.0   0.0   0.0  0.140033  0.050281   \n",
      "\n",
      "        NIVEL_RELACIONAMENTO_CREDITO01    ...       CEP4_5    CEP4_6  \\\n",
      "0                             0.111111    ...     1.000000  0.000000   \n",
      "1                             0.111111    ...     0.000000  0.000000   \n",
      "2                             0.111111    ...     1.000000  0.000000   \n",
      "3                             0.111111    ...     1.000000  0.000000   \n",
      "4                             0.111111    ...     1.000000  0.000000   \n",
      "5                             0.111111    ...     0.000000  1.000000   \n",
      "6                             0.111111    ...     0.000000  0.000000   \n",
      "7                             0.111111    ...     1.000000  0.000000   \n",
      "8                             0.111111    ...     1.000000  1.000000   \n",
      "9                             0.111111    ...     0.000000  1.000000   \n",
      "10                            0.111111    ...     0.000000  1.000000   \n",
      "11                            0.111111    ...     0.000000  0.000000   \n",
      "12                            0.111111    ...     0.000000  0.000000   \n",
      "13                            0.777778    ...     0.000000  0.000000   \n",
      "14                            0.111111    ...     0.000000  1.000000   \n",
      "15                            0.111111    ...     1.000000  0.000000   \n",
      "16                            0.111111    ...     0.000000  0.000000   \n",
      "17                            0.111111    ...     1.000000  1.000000   \n",
      "18                            0.111111    ...     1.000000  0.000000   \n",
      "19                            0.111111    ...     0.000000  1.000000   \n",
      "20                            0.111111    ...     0.000000  1.000000   \n",
      "21                            0.222222    ...     0.000000  0.000000   \n",
      "22                            0.111111    ...     0.000000  0.000000   \n",
      "23                            0.111111    ...     0.000000  0.000000   \n",
      "24                            0.111111    ...     1.000000  0.000000   \n",
      "25                            0.111111    ...     0.000000  1.000000   \n",
      "26                            0.111111    ...     0.000000  0.000000   \n",
      "27                            0.111111    ...     1.000000  1.000000   \n",
      "28                            0.111111    ...     0.000000  0.000000   \n",
      "29                            0.111111    ...     1.000000  1.000000   \n",
      "...                                ...    ...          ...       ...   \n",
      "255068                        0.111111    ...     0.000000  0.000000   \n",
      "255069                        0.111111    ...     0.431645  0.000000   \n",
      "255070                        0.111111    ...     0.495644  0.504356   \n",
      "255071                        0.111111    ...     0.000000  0.000000   \n",
      "255072                        0.111111    ...     0.000000  1.000000   \n",
      "255073                        0.111111    ...     1.000000  0.000000   \n",
      "255074                        0.111111    ...     1.000000  1.000000   \n",
      "255075                        0.111111    ...     1.000000  1.000000   \n",
      "255076                        0.111111    ...     0.000000  0.000000   \n",
      "255077                        0.111111    ...     0.000000  0.000000   \n",
      "255078                        0.111111    ...     0.665863  1.000000   \n",
      "255079                        0.111111    ...     1.000000  1.000000   \n",
      "255080                        0.111111    ...     0.000000  1.000000   \n",
      "255081                        0.111111    ...     0.000000  1.000000   \n",
      "255082                        0.111111    ...     1.000000  0.000000   \n",
      "255083                        0.111111    ...     1.000000  0.000000   \n",
      "255084                        0.111111    ...     1.000000  0.000000   \n",
      "255085                        0.111111    ...     0.000000  0.000000   \n",
      "255086                        0.111111    ...     0.000000  1.000000   \n",
      "255087                        0.111111    ...     0.000000  0.000000   \n",
      "255088                        0.111111    ...     1.000000  0.000000   \n",
      "255089                        0.111111    ...     1.000000  0.000000   \n",
      "255090                        0.111111    ...     0.570825  0.429175   \n",
      "255091                        0.111111    ...     0.000000  0.000000   \n",
      "255092                        0.111111    ...     0.014315  0.000000   \n",
      "255093                        0.111111    ...     0.000000  0.000000   \n",
      "255094                        0.111111    ...     0.000000  0.733497   \n",
      "255095                        0.345691    ...     0.000000  0.000000   \n",
      "255096                        0.111111    ...     1.000000  1.000000   \n",
      "255097                        0.111111    ...     0.949719  0.000000   \n",
      "\n",
      "          CEP4_7    CEP4_8    CEP4_9   CEP4_10   CEP4_11   CEP4_12   CEP4_13  \\\n",
      "0       0.000000  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "1       0.000000  0.000000  1.000000  0.000000  1.000000  1.000000  1.000000   \n",
      "2       1.000000  1.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
      "3       0.000000  1.000000  1.000000  1.000000  0.000000  0.000000  1.000000   \n",
      "4       0.000000  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000   \n",
      "5       0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000   \n",
      "6       0.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "7       0.000000  1.000000  0.000000  1.000000  1.000000  1.000000  0.000000   \n",
      "8       1.000000  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "9       1.000000  0.000000  1.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "10      0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "11      0.000000  1.000000  1.000000  0.000000  1.000000  1.000000  1.000000   \n",
      "12      0.000000  1.000000  0.000000  1.000000  1.000000  0.000000  1.000000   \n",
      "13      0.000000  0.000000  0.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "14      0.000000  0.000000  1.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "15      1.000000  1.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "16      0.000000  1.000000  0.000000  1.000000  1.000000  0.000000  0.000000   \n",
      "17      1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  0.000000   \n",
      "18      0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "19      1.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
      "20      1.000000  0.000000  0.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "21      1.000000  0.000000  0.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "22      1.000000  1.000000  0.000000  1.000000  0.000000  0.000000  1.000000   \n",
      "23      0.000000  1.000000  1.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "24      0.000000  1.000000  1.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "25      0.000000  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000   \n",
      "26      0.000000  0.000000  1.000000  1.000000  0.000000  1.000000  1.000000   \n",
      "27      0.000000  1.000000  1.000000  1.000000  0.000000  0.000000  0.000000   \n",
      "28      1.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "29      0.000000  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "255068  0.000000  1.000000  1.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "255069  0.431645  1.000000  0.000000  1.000000  1.000000  0.431645  0.568355   \n",
      "255070  0.000000  0.000000  1.000000  0.000000  0.504356  1.000000  0.495644   \n",
      "255071  0.000000  0.000000  1.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "255072  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000  0.000000   \n",
      "255073  0.000000  1.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
      "255074  0.000000  0.000000  0.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "255075  0.000000  1.000000  0.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "255076  0.000000  1.000000  1.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "255077  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  1.000000   \n",
      "255078  1.000000  0.000000  0.000000  0.000000  0.000000  0.334137  0.000000   \n",
      "255079  1.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "255080  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000  0.000000   \n",
      "255081  0.000000  0.000000  1.000000  1.000000  1.000000  1.000000  0.000000   \n",
      "255082  0.000000  1.000000  1.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "255083  1.000000  0.000000  0.000000  1.000000  1.000000  0.000000  1.000000   \n",
      "255084  1.000000  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000   \n",
      "255085  1.000000  0.000000  0.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "255086  1.000000  0.000000  0.000000  1.000000  1.000000  0.000000  1.000000   \n",
      "255087  0.000000  1.000000  0.000000  0.000000  1.000000  0.000000  1.000000   \n",
      "255088  0.000000  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "255089  1.000000  1.000000  0.000000  0.000000  1.000000  0.000000  0.000000   \n",
      "255090  0.429175  1.000000  0.570825  1.000000  0.429175  0.000000  0.000000   \n",
      "255091  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  1.000000   \n",
      "255092  0.000000  0.014315  1.000000  0.985685  0.000000  1.000000  1.000000   \n",
      "255093  0.000000  0.000000  1.000000  0.000000  1.000000  1.000000  1.000000   \n",
      "255094  1.000000  0.000000  1.000000  0.000000  0.733497  0.266503  0.000000   \n",
      "255095  0.000000  0.000000  0.000000  1.000000  1.000000  0.000000  1.000000   \n",
      "255096  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000   \n",
      "255097  1.000000  1.000000  1.000000  0.050281  1.000000  1.000000  0.000000   \n",
      "\n",
      "         CEP4_14  \n",
      "0       1.000000  \n",
      "1       0.000000  \n",
      "2       1.000000  \n",
      "3       0.000000  \n",
      "4       0.000000  \n",
      "5       1.000000  \n",
      "6       0.000000  \n",
      "7       0.000000  \n",
      "8       0.000000  \n",
      "9       1.000000  \n",
      "10      1.000000  \n",
      "11      0.000000  \n",
      "12      0.000000  \n",
      "13      0.000000  \n",
      "14      0.000000  \n",
      "15      1.000000  \n",
      "16      1.000000  \n",
      "17      0.000000  \n",
      "18      1.000000  \n",
      "19      1.000000  \n",
      "20      0.000000  \n",
      "21      0.000000  \n",
      "22      1.000000  \n",
      "23      0.000000  \n",
      "24      0.000000  \n",
      "25      0.000000  \n",
      "26      0.000000  \n",
      "27      0.000000  \n",
      "28      0.000000  \n",
      "29      0.000000  \n",
      "...          ...  \n",
      "255068  0.000000  \n",
      "255069  0.568355  \n",
      "255070  0.000000  \n",
      "255071  1.000000  \n",
      "255072  1.000000  \n",
      "255073  1.000000  \n",
      "255074  0.000000  \n",
      "255075  1.000000  \n",
      "255076  0.000000  \n",
      "255077  0.000000  \n",
      "255078  0.334137  \n",
      "255079  0.000000  \n",
      "255080  0.000000  \n",
      "255081  1.000000  \n",
      "255082  0.000000  \n",
      "255083  0.000000  \n",
      "255084  0.000000  \n",
      "255085  0.000000  \n",
      "255086  0.000000  \n",
      "255087  1.000000  \n",
      "255088  1.000000  \n",
      "255089  1.000000  \n",
      "255090  0.000000  \n",
      "255091  1.000000  \n",
      "255092  0.985685  \n",
      "255093  1.000000  \n",
      "255094  0.733497  \n",
      "255095  0.000000  \n",
      "255096  1.000000  \n",
      "255097  0.000000  \n",
      "\n",
      "[255098 rows x 243 columns]\n"
     ]
    }
   ],
   "source": [
    "## TO DO -- Implementar as funções de sampling a serem utilizadas\n",
    "\n",
    "y_train_group = y_train.groupby('IND_BOM_1_1')\n",
    "print(y_train_group.size())\n",
    "\n",
    "X_train_columns = X_train.columns\n",
    "\n",
    "# oversampling nos dados de treinamento\n",
    "X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "print(\"Base de treinamento\")\n",
    "print(\"Class 0: \", np.where(y_train == 0)[0].shape[0])\n",
    "print(\"Class 1: \", np.where(y_train == 1)[0].shape[0])\n",
    "\n",
    "y_train = pd.DataFrame(np.array(y_train), columns=['IND_BOM_1_1'])\n",
    "X_train = pd.DataFrame(np.array(X_train), columns=X_train_columns)\n",
    "\n",
    "# oversampling nos dados de validação\n",
    "X_val, y_val = SMOTE().fit_sample(X_val, y_val)\n",
    "print(\"Base de validação\")\n",
    "print(\"Class 0: \", np.where(y_val == 0)[0].shape[0])\n",
    "print(\"Class 1: \", np.where(y_val == 1)[0].shape[0])\n",
    "\n",
    "y_val = pd.DataFrame(np.array(y_val), columns=['IND_BOM_1_1'])\n",
    "X_val = pd.DataFrame(np.array(X_val), columns=X_train_columns)\n",
    "\n",
    "\n",
    "#X_train.describe()\n",
    "df_train = pd.DataFrame(X_train)\n",
    "print(X_train)\n",
    "#f_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante lembrar de normalizar os dados. A classe `StandardScaler` centraliza as variáveis e transforma as features para terem variância unitária. Você pode testar outras opções como o `MinMaxScaler`.\n",
    "\n",
    "Todas as alternativas estão disponíveis em:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.49166856  0.37156596  0.69023372  1.08082579 -0.67418088 -0.59435622\n",
      " -0.53943671 -0.49302891  1.84898557  0.99697037 -0.19378465 -0.05108644\n",
      " -0.2865236  -0.18298907 -0.13000445 -0.12569987 -0.11302087  0.28099599\n",
      "  0.29417237 -0.53289452 -0.3518387  -0.35073559 -0.4654444  -0.27995596\n",
      "  0.80172009 -0.4260305  -0.53347845 -0.32970784 -0.28231018 -0.42173103\n",
      " -0.54757364  0.20167484  0.36166907 -2.50669569 -0.3188381  -0.29851785\n",
      "  3.81120397 -0.26270127  1.91472698  0.88553597 -0.30268525  0.18478571\n",
      " -0.17845119  0.29844535  0.06250679  0.20430941  0.99897485 -0.70196169\n",
      " -0.42199684 -0.26456887 -0.55166001  0.30286956 -0.28643221  0.53923624\n",
      " -0.4416035  -0.38006221 -0.559879    0.68208793 -0.31262546  1.31537284\n",
      " -1.0105708   1.22180807  1.15065353  1.16618445 -0.94019606  0.88316789\n",
      " -0.63089207 -1.16395216 -0.90057129  0.44243528 -0.90007812 -1.16412516\n",
      "  1.50416235 -0.78365298 -0.72327325 -0.81072464  0.426778    1.03683123\n",
      " -0.85555573 -0.43048398 -0.76044742 -1.11481127  0.23029466 -0.05445517\n",
      "  1.29147705 -0.9660655   0.58276136  1.25941687 -0.94306699  0.07048042\n",
      " -0.6970957   1.01302509 -1.13146168  1.58627884  0.25632813 -0.40580883\n",
      " -0.71078763  1.07417547 -0.72407841 -0.51105988 -0.0552386   1.25033988\n",
      "  1.24673964 -0.05685331  0.31363504  1.10769264  1.03288652 -0.72707398\n",
      "  0.93313493  0.45673911  1.61565463 -0.39356447 -0.36510451 -0.41259272\n",
      "  1.18249321  1.1747335   0.67168031 -0.43712127 -0.37545886 -0.49896059\n",
      " -0.29441617 -0.44012787  0.57855667 -0.06704916 -0.46480584 -0.35906117\n",
      " -0.41764381 -0.38475253 -0.29482029 -0.49650045  0.33651669 -0.52496646\n",
      "  0.37471734  1.85502313 -0.34730127  0.4342258   0.25185204 -0.15038546\n",
      " -0.31947452 -0.48099409  1.41124825  0.676418    0.05313138  1.65402437\n",
      "  1.18337691  1.32732579  0.32060681 -0.33904156 -0.67959607 -0.45354043\n",
      " -0.69527477 -0.69916111 -0.61506192 -0.4061933  -0.80359615 -0.69164018\n",
      " -0.64891488 -0.74674637 -0.46102453 -0.76667934 -0.32573219 -0.721602\n",
      " -0.49998195 -0.59384938 -0.65348613 -0.63044805 -0.76489258 -0.69036605\n",
      " -0.59901728 -0.70002656 -0.62553452 -0.44183147 -0.80117888 -0.76870763\n",
      " -0.60475518 -0.76812592 -0.72281274 -0.70351788 -0.62535819 -0.3891452\n",
      " -0.47658632 -0.43611337 -0.66509836  0.50843936 -0.34459311 -0.33517624\n",
      " -0.72205415 -0.37462898 -0.55624733 -0.33050581 -0.58757351 -0.28940236\n",
      " -0.32898197 -0.39330579 -0.33850316 -0.35329317 -0.52708151 -0.43361267\n",
      " -0.35884079 -0.66453173 -0.33201001 -0.42454639 -0.31093289 -0.26659947\n",
      "  1.09234585 -0.89351818  1.31502488 -0.78486932 -0.76744686  0.53966141\n",
      "  0.8723475  -0.96940183 -0.87964863  1.34570839 -0.77144529  1.43791381\n",
      " -0.7475264  -0.73023559 -1.82028659  0.8409396   1.1470754  -0.84457441\n",
      "  1.33818026 -0.73980159 -0.73198099  1.520142   -0.75771755 -0.73706611\n",
      " -0.80179786  1.33796299  0.50528221  0.69720218 -1.27217223 -1.06844394\n",
      "  1.09542076 -0.91645074 -0.88842842  1.22926354 -0.90119035  1.12763531\n",
      "  1.17577718  1.18322536 -0.91030057 -0.92463489]\n",
      "Original:\n",
      "[ 2.69400000e+04  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  2.77555756e-17  0.00000000e+00\n",
      "  8.90226392e-01  1.00000000e+00  1.11111111e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.73472348e-18\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  1.38777878e-17\n",
      "  4.16666667e-02  0.00000000e+00  0.00000000e+00  1.47356127e-01\n",
      "  8.50630481e-01 -5.55111512e-17  1.38777878e-17  1.00000000e-01\n",
      "  1.38821155e-01  0.00000000e+00  2.77555756e-17  1.00000000e+00\n",
      "  1.00000000e+00 -1.11022302e-16 -1.38777878e-17  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  3.46944695e-18  1.00000000e+00\n",
      "  1.48053641e-01  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.93889390e-18  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  6.93889390e-18  0.00000000e+00\n",
      "  0.00000000e+00  7.54710000e-01  3.15538000e-01  9.52002000e-01\n",
      "  1.70620000e-02  8.95741000e-01  9.97024000e-01  8.65771000e-01\n",
      "  7.28190000e-02  9.98797000e-01  2.62265000e-01  6.90960000e-02\n",
      "  1.64400000e-03  6.02831000e-01  8.02050000e-02  2.26810000e-02\n",
      "  9.97213000e-01  1.02450000e-02  1.44131000e-01  1.58170000e-01\n",
      "  5.73276000e-01  8.65786000e-01  3.47460000e-02  3.32455000e-01\n",
      "  1.61581000e-01  8.19380000e-02  4.57756000e-01  4.04610000e-01\n",
      "  9.98182000e-01  3.87500000e-03  6.20590000e-01  8.15757000e-01\n",
      "  1.79663000e-01  5.03264000e-01  1.21336000e-01  9.90988000e-01\n",
      "  3.84800000e-03  9.85169000e-01  6.53808000e-01  3.08830000e-01\n",
      "  1.55368000e-01  9.99963000e-01  1.91833000e-01  2.41716000e-01\n",
      "  4.81397000e-01  9.48699000e-01  9.67093000e-01  4.64436000e-01\n",
      "  6.37716000e-01  9.82114000e-01  8.59262000e-01  1.78808000e-01\n",
      "  7.57215000e-01  2.58910083e-01  4.11827738e-01  3.22104063e-02\n",
      "  3.96622365e-02  2.54237454e-02  3.60224915e-01  3.31474477e-01\n",
      "  2.71042261e-01  1.72575666e-02  3.49830120e-02  2.12808762e-02\n",
      "  3.80501487e-02  3.46399565e-02  2.26034829e-01  1.37598767e-01\n",
      "  2.03639678e-02  3.98580287e-02  2.04437794e-02  2.72773988e-02\n",
      "  3.17624818e-02  4.15128284e-03  1.47670508e-01  2.04945984e-02\n",
      "  2.03011792e-01  4.54442852e-01  4.78667911e-02  1.79798805e-01\n",
      "  1.56345674e-01  7.64272133e-02  4.17671908e-02  1.63577773e-02\n",
      "  1.00000000e+00  2.39661495e-01  2.87163388e-01  4.63619225e-01\n",
      "  5.36595173e-01  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.76497696e-02  1.11111111e-02  2.21238938e-02\n",
      "  1.22448980e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.73010381e-02  4.76190476e-03  6.06060606e-02  2.02020202e-02\n",
      "  1.01449275e-01  0.00000000e+00  0.00000000e+00  3.17460317e-02\n",
      "  5.08474576e-02  3.78548896e-02  1.07142857e-02  3.41296928e-03\n",
      "  9.34579439e-03  0.00000000e+00  2.00803213e-02  0.00000000e+00\n",
      "  0.00000000e+00  2.17391304e-02 -1.38777878e-17  0.00000000e+00\n",
      "  0.00000000e+00  3.47826087e-02  4.31034483e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.38777878e-17  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.77555756e-17  0.00000000e+00  0.00000000e+00  5.90059006e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.93889390e-18\n",
      "  1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição e Treino da Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos a arquitetura da nossa rede neural e treinamos ela.\n",
    "\n",
    "No presente exemplo a rede possui apenas uma camada escondida. O código é bem intuitivo e a adição de novas camadas pode ser feita através da função `add`.\n",
    "\n",
    "Para treinar a rede várias funções de otimização estão disponíveis. \n",
    "\n",
    "Confira os exemplos em: https://keras.io/optimizers/\n",
    "\n",
    "O treinamento da rede pode ser interrompido baseado na performance dela em um conjunto de validação através de callbacks.\n",
    "\n",
    "Confira a documentação da classe `EarlyStopping`: https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de features do nosso data set.\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Aqui criamos o esboço da rede.\n",
    "classifier = Sequential()\n",
    "\n",
    "# Agora adicionamos a primeira camada escondida contendo 16 neurônios e função de ativação\n",
    "# tangente hiperbólica. Por ser a primeira camada adicionada à rede, precisamos especificar\n",
    "# a dimensão de entrada (número de features do data set).\n",
    "classifier.add(Dense(16, activation='tanh', input_dim=input_dim))\n",
    "\n",
    "# Em seguida adicionamos a camada de saída. Como nosso problema é binário só precisamos de\n",
    "# 1 neurônio com função de ativação sigmoidal. A partir da segunda camada adicionada keras já\n",
    "# consegue inferir o número de neurônios de entrada (16) e nós não precisamos mais especificar.\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Por fim compilamos o modelo especificando um otimizador, a função de custo, e opcionalmente\n",
    "# métricas para serem observadas durante treinamento.\n",
    "classifier.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3924 samples, validate on 1962 samples\n",
      "Epoch 1/100000\n",
      "3924/3924 [==============================] - 1s 369us/step - loss: 0.2219 - val_loss: 0.1736\n",
      "Epoch 2/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.1418 - val_loss: 0.1149\n",
      "Epoch 3/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0925 - val_loss: 0.0745\n",
      "Epoch 4/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0603 - val_loss: 0.0506\n",
      "Epoch 5/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0423 - val_loss: 0.0378\n",
      "Epoch 6/100000\n",
      "3924/3924 [==============================] - 0s 32us/step - loss: 0.0327 - val_loss: 0.0310\n",
      "Epoch 7/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0274 - val_loss: 0.0271\n",
      "Epoch 8/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0242 - val_loss: 0.0248\n",
      "Epoch 9/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 10/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 11/100000\n",
      "3924/3924 [==============================] - 0s 53us/step - loss: 0.0197 - val_loss: 0.0214\n",
      "Epoch 12/100000\n",
      "3924/3924 [==============================] - 0s 41us/step - loss: 0.0189 - val_loss: 0.0207\n",
      "Epoch 13/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0183 - val_loss: 0.0202\n",
      "Epoch 14/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0178 - val_loss: 0.0199\n",
      "Epoch 15/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 16/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0170 - val_loss: 0.0192\n",
      "Epoch 17/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0167 - val_loss: 0.0190\n",
      "Epoch 18/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0165 - val_loss: 0.0188\n",
      "Epoch 19/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 20/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0161 - val_loss: 0.0184\n",
      "Epoch 21/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0158 - val_loss: 0.0183\n",
      "Epoch 22/100000\n",
      "3924/3924 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0182\n",
      "Epoch 23/100000\n",
      "3924/3924 [==============================] - 0s 51us/step - loss: 0.0155 - val_loss: 0.0180\n",
      "Epoch 24/100000\n",
      "3924/3924 [==============================] - 0s 48us/step - loss: 0.0154 - val_loss: 0.0179\n",
      "Epoch 25/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 26/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0152 - val_loss: 0.0177\n",
      "Epoch 27/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0150 - val_loss: 0.0176\n",
      "Epoch 28/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0149 - val_loss: 0.0175\n",
      "Epoch 29/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0148 - val_loss: 0.0174\n",
      "Epoch 30/100000\n",
      "3924/3924 [==============================] - 0s 36us/step - loss: 0.0146 - val_loss: 0.0173\n",
      "Epoch 31/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0146 - val_loss: 0.0172\n",
      "Epoch 32/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0145 - val_loss: 0.0171\n",
      "Epoch 33/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0171\n",
      "Epoch 34/100000\n",
      "3924/3924 [==============================] - 0s 44us/step - loss: 0.0143 - val_loss: 0.0170\n",
      "Epoch 35/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0142 - val_loss: 0.0169\n",
      "Epoch 36/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0142 - val_loss: 0.0169\n",
      "Epoch 37/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 38/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0139 - val_loss: 0.0167\n",
      "Epoch 39/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0139 - val_loss: 0.0167\n",
      "Epoch 40/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0138 - val_loss: 0.0166\n",
      "Epoch 41/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0138 - val_loss: 0.0166\n",
      "Epoch 42/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0137 - val_loss: 0.0165\n",
      "Epoch 43/100000\n",
      "3924/3924 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0166\n",
      "Epoch 44/100000\n",
      "3924/3924 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0165\n",
      "Epoch 45/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 46/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 47/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0134 - val_loss: 0.0164\n",
      "Epoch 48/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0134 - val_loss: 0.0163\n",
      "Epoch 49/100000\n",
      "3924/3924 [==============================] - 0s 41us/step - loss: 0.0133 - val_loss: 0.0163\n",
      "Epoch 50/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 51/100000\n",
      "3924/3924 [==============================] - 0s 51us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 52/100000\n",
      "3924/3924 [==============================] - 0s 43us/step - loss: 0.0132 - val_loss: 0.0162\n",
      "Epoch 53/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 54/100000\n",
      "3924/3924 [==============================] - 0s 40us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 55/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 56/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0130 - val_loss: 0.0161\n",
      "Epoch 57/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0129 - val_loss: 0.0162\n",
      "Epoch 58/100000\n",
      "3924/3924 [==============================] - 0s 35us/step - loss: 0.0128 - val_loss: 0.0160\n",
      "Epoch 59/100000\n",
      "3924/3924 [==============================] - 0s 33us/step - loss: 0.0128 - val_loss: 0.0161\n",
      "Epoch 60/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 61/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 62/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 63/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0127 - val_loss: 0.0159\n",
      "Epoch 64/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 65/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0126 - val_loss: 0.0160\n",
      "Epoch 66/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0127 - val_loss: 0.0159\n",
      "Epoch 67/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 68/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 69/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 70/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 71/100000\n",
      "3924/3924 [==============================] - 0s 42us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 72/100000\n",
      "3924/3924 [==============================] - 0s 37us/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 73/100000\n",
      "3924/3924 [==============================] - 0s 39us/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 74/100000\n",
      "3924/3924 [==============================] - 0s 46us/step - loss: 0.0124 - val_loss: 0.0157\n",
      "Epoch 75/100000\n",
      "3924/3924 [==============================] - 0s 45us/step - loss: 0.0124 - val_loss: 0.0157\n",
      "Epoch 76/100000\n",
      "3924/3924 [==============================] - 0s 38us/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 77/100000\n",
      "3924/3924 [==============================] - 0s 32us/step - loss: 0.0123 - val_loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "# Para treinar a rede passamos o conjunto de treinamento e especificamos o tamanho do mini-batch,\n",
    "# o número máximo de épocas, e opcionalmente callbacks. No seguinte exemplo utilizamos early\n",
    "# stopping para interromper o treinamento caso a performance não melhore em um conjunto de validação.\n",
    "history = classifier.fit(X_train, y_train, batch_size=64, epochs=100000, \n",
    "                         callbacks=[EarlyStopping(patience=3)], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas funções auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o melhor loss de treino e validação.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    Dicionário contendo o melhor loss de treino e de validação baseado \n",
    "    no menor loss de validação.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    idx_min_val_loss = np.argmin(val_loss)\n",
    "    return {'train_loss': train_loss[idx_min_val_loss], 'val_loss': val_loss[idx_min_val_loss]}\n",
    "\n",
    "def plot_training_error_curves(history):\n",
    "    \"\"\"Função para plotar as curvas de erro do treinamento da rede neural.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    A função gera o gráfico do treino da rede e retorna None.\n",
    "    \"\"\"\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label='Train')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.set(title='Training and Validation Error Curves', xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr)\n",
    "    return performance_metrics\n",
    "\n",
    "def print_metrics_summary(accuracy, recall, precision, f1, auroc=None, aupr=None):\n",
    "    print()\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision))\n",
    "    print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1))\n",
    "    if auroc is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=auroc))\n",
    "    if aupr is not None:\n",
    "        print(\"{metric:<18}{value:.4f}\".format(metric=\"AUPR:\", value=aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_error_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
